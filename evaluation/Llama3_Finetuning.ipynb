{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries if not already installed"
      ],
      "metadata": {
        "id": "xidz5c3xu4wb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsxtmrgBtUBS"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate peft bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "O9RBm0MMvAeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import LlamaTokenizer, LlamaModel, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, SFTTrainer, TaskType\n",
        "from bitsandbytes import BitsAndBytesConfig\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "FPurGsFUvG_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check device"
      ],
      "metadata": {
        "id": "Vw8EjhNJvIdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "-YwexZgdvPH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "DKVIvlDPvQQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stsb_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")"
      ],
      "metadata": {
        "id": "ugKRAdY6vZFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer and Model setup"
      ],
      "metadata": {
        "id": "XWojgIbovawq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3B\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "model = LlamaModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "k3rWit7evjmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the LoRA fine-tuning configuration"
      ],
      "metadata": {
        "id": "PpY2r0ytvvjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_r = 4\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "peft_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLASSIFICATION  # change as per task\n",
        ")"
      ],
      "metadata": {
        "id": "afdEDQG_vz-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring quantization and bits-and-bytes for efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the function for tokenization\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(batch[\"sentence1\"], batch[\"sentence2\"], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# Tokenize the dataset\n",
        "stsb_dataset = stsb_dataset.map(tokenize_batch, batched=True)\n",
        "\n",
        "# Define the metric computation function for similarity\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
        "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Fine-tuning with Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=stsb_dataset[\"train\"],\n",
        "    eval_dataset=stsb_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Run training\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate on test set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "# Saving the fine-tuned model for later comparisons\n",
        "model.save_pretrained(\"./fine_tuned_llama\")"
      ],
      "metadata": {
        "id": "m4_xN2g2wA_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### compare the performance of the fine-tuned Llama model versus the non-fine-tuned version"
      ],
      "metadata": {
        "id": "fAOe_YLcwNNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a new instance of the non-fine-tuned model for comparison\n",
        "non_fine_tuned_model = LlamaModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Loading a sample classification dataset for comparison (using the STSb test set)\n",
        "stsb_test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
        "\n",
        "# Tokenize the test dataset for evaluation\n",
        "stsb_test_dataset = stsb_test_dataset.map(tokenize_batch, batched=True)\n",
        "\n",
        "# Define an evaluation function to get similarity scores from models\n",
        "def evaluate_model(model, dataset):\n",
        "    model.eval()\n",
        "    similarities = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataset:\n",
        "            inputs = tokenizer(\n",
        "                batch[\"sentence1\"],\n",
        "                batch[\"sentence2\"],\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=128\n",
        "            ).to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            # Assuming that we use the CLS token's hidden state for similarity\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "            similarity_scores = torch.cosine_similarity(\n",
        "                embeddings[0::2], embeddings[1::2], dim=1\n",
        "            )\n",
        "            similarities.extend(similarity_scores.cpu().numpy())\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Evaluate the fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "fine_tuned_similarities = evaluate_model(model, stsb_test_dataset)\n",
        "\n",
        "# Evaluate the non-fine-tuned model\n",
        "print(\"Evaluating non-fine-tuned model...\")\n",
        "non_fine_tuned_similarities = evaluate_model(non_fine_tuned_model, stsb_test_dataset)\n",
        "\n",
        "# Compare the results with ground truth similarity scores from the dataset\n",
        "ground_truth = stsb_test_dataset[\"similarity_score\"]  # Assuming 'similarity_score' is the correct field\n",
        "\n",
        "# Compute correlation metrics (e.g., Spearman's correlation)\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "fine_tuned_correlation = spearmanr(fine_tuned_similarities, ground_truth).correlation\n",
        "non_fine_tuned_correlation = spearmanr(non_fine_tuned_similarities, ground_truth).correlation\n",
        "\n",
        "print(f\"Spearman's correlation for the fine-tuned model: {fine_tuned_correlation:.4f}\")\n",
        "print(f\"Spearman's correlation for the non-fine-tuned model: {non_fine_tuned_correlation:.4f}\")\n",
        "\n",
        "# Interpretation of results\n",
        "if fine_tuned_correlation > non_fine_tuned_correlation:\n",
        "    print(\"The fine-tuned model shows better alignment with human-rated scores, indicating successful fine-tuning.\")\n",
        "else:\n",
        "    print(\"The non-fine-tuned model performs comparably or better, suggesting that further tuning strategies may be needed.\")\n"
      ],
      "metadata": {
        "id": "CvhFls9XwU3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}